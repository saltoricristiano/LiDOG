<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="keywords" content="LiDOG">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
   <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://saltoricristiano.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/saltoricristiano/cosmix-uda">
            CoSMiX
          </a>
          <a class="navbar-item" href="https://github.com/saltoricristiano/gipso-sfouda">
            GIPSO
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Walking Your LiDOG&#128054;: A Journey Through Multiple Domains for LiDAR Semantic Segmentation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://saltoricristiano.github.io">Cristiano Saltori</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://aljosaosep.github.io">Aljoša Ošep</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.ca/citations?user=xf1T870AAAAJ&hl=en">Elisa Ricci</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=tT2TC-UAAAAJ&hl=en">Laura Leal-Taixé</a><sup>4</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Trento,</span>
            <span class="author-block"><sup>2</sup>TU Munich,</span>
            <span class="author-block"><sup>3</sup>Federazione Bruno Kessler</span>
            <span class="author-block"><sup>4</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2304.11705"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.11705"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/IKOppoT8cIE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/saltoricristiano/ligodcode"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <img src="static/images/teaser.png" alt="Teaser">
          <p>
            Existing LSS methods are trained and evaluated on point clouds drawn from the same domain (<i>left</i>). We focus on studying LSS under domain shifts, where the test samples are drawn from a different data distribution (<i>right</i>).
            Our paper aims to address the generalization aspect of this task.
          </p>
        </div>

      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Explanatory video</h2>
        <div class="publication-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/IKOppoT8cIE?si=WPWbDQcqtE7J-6sQ" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain \textit{LiDAR semantic segmentation}.
            However, do these methods generalize \textit{across} domains?
          </p>
          <p>
            To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS).
            Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains 26.53 mIoU on the target data, compared to 48.49 mIoU obtained by the model trained on the target domain (nuScenes).
          </p>
          <p>
            To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains 34.88 mIoU on the target domain, outperforming all baselines.
            Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional decoder that learns to classify a birds-eye view of the point cloud.
            This simple auxiliary task encourages the 3D network to learn features that are robust to sensor placement shifts and resolution, and are transferable across domains.
          </p>
          <p>
            With this work, we aim to inspire the community to develop and evaluate future models in such cross-domain conditions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">LiDAR DOmain Generalization (LiDOG &#128054;)</h2>
        <img src="static/images/lidog.png" alt="Method figure">
      </div>
    </div>
    <!--/ Paper video. -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">

          <p>
            We encode our input LiDAR scan $P_j$ using the 3D backbone $g^{3D}$ to learn the occupied voxels' feature representations $F^{3D}$.
            (<i> Upper branch - main task </i>) We apply a sparse segmentation head on $F^{3D}$ and supervise with 3D semantic labels, $\mathcal{Y}_j^{3D}$.
            (<i> Lower branch - auxiliary task </i>) We project those features along the height-axis to obtain a dense 2D bird's-eye (BEV) view features $F^{BEV}$, and apply several 2D convolutional layers to learn the 2D BEV representation. We supervise the BEV auxiliary task by using BEV-view of semantic labels, $\mathcal{Y}_j^{BEV}$.
            We train jointly on both $L^{3D}$ and $L^{BEV}$.}
          </p>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{saltori2023walking,
  title={Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation},
  author={Saltori, Cristiano and Osep, Aljosa and Ricci, Elisa and Leal-Taix{\'e}, Laura},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={196--206},
  year={2023}
}</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgments">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgments</h2>
    <p>
      This project was partially funded by the Sofja Kovalevskaja Award of the Humboldt Foundation, the EU ISFP project PRECRISIS (ISFP-2022-TFI-AG-PROTECT-02-101100539), the PRIN project LEGO-AI (Prot. 2020TA3K9N) and the MUR PNRR project FAIR - Future AI Research (PE00000013) funded by the NextGenerationEU.
      It was carried out in the Vision and Learning joint laboratory of FBK-UNITN and used the CINECA, NVIDIA-AI TC clusters to run part of the experiments.
      We thank T. Meinhardt and I. Elezi for their feedback on this manuscript. The authors of this work take full responsibility for its content.

    </p>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. This wepage template is from <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a>.
            We sincerely thank <a href="https://keunhong.com">Keunhong Park</a> for developing and open-sourcing this template.

          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
